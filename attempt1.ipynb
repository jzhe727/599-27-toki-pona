{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sentence pairs in English-Toki Pona - 2024-04-02.tsv'   src-eval.txt\n",
      " attempt1.ipynb                                          src-train.txt\n",
      " broken_tgt_nor_in.vocab                                 src-val.txt\n",
      " config.yaml                                             src.txt\n",
      " from_scratch.slurm                                      tgt-eval.txt\n",
      " \u001b[0m\u001b[01;34mmodels\u001b[0m/                                                 tgt-train.txt\n",
      " \u001b[01;34mmodels_early_stop\u001b[0m/                                      tgt-val.txt\n",
      " slurm-28958415.out                                      tgt.txt\n",
      " slurm-28958627.out                                      train.log\n",
      " slurm-28958779.out\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def remove_punct(s: str):\n",
    "    for punct in (string.punctuation + '\\t'):\n",
    "        s = s.replace(punct, \"\")\n",
    "        s = s.replace(\"  \", \" \")\n",
    "        s = s.replace(chr(160),\" \") # get rid of the stupid no-break space\n",
    "\n",
    "    return(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/chapter/10.1007/978-3-031-36616-1_52\n",
    "\n",
    "Base Model configs\n",
    "\n",
    "![](https://media.springernature.com/lw368/springer-static/image/chp%3A10.1007%2F978-3-031-36616-1_52/MediaObjects/539942_1_En_52_Tab2_HTML.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = '''# config_base_model.yaml\n",
    "\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: run\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    corpus_1:\n",
    "        path_src: src-train.txt\n",
    "        path_tgt: tgt-train.txt\n",
    "    valid:\n",
    "        path_src: src-val.txt\n",
    "        path_tgt: tgt-val.txt\n",
    "\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: src.vocab\n",
    "tgt_vocab: tgt.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 50000\n",
    "tgt_vocab_size: 50000\n",
    "\n",
    "# Filter out source/target longer than n if [filtertoolong] enabled\n",
    "src_seq_length: 150\n",
    "src_seq_length: 150\n",
    "\n",
    "# Tokenization options\n",
    "src_subword_model: source.model\n",
    "tgt_subword_model: target.model\n",
    "\n",
    "# Where to save the log file and the output models/checkpoints\n",
    "log_file: train.log\n",
    "save_model: models_scratch/modelv1\n",
    "\n",
    "# Stop training if it does not imporve after n validations\n",
    "# early_stopping: 4\n",
    "\n",
    "# Default: 5000 - Save a model checkpoint for each n\n",
    "save_checkpoint_steps: 1000\n",
    "\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 3435\n",
    "\n",
    "# Default: 100000 - Train the model to max n steps\n",
    "# Increase to 200000 or more for large datasets\n",
    "# For fine-tuning, add up the required steps to the original steps\n",
    "# decrease steps \n",
    "train_steps: 5000\n",
    "\n",
    "# # Default: 10000 - Run validation after n steps\n",
    "# # once every 78 \n",
    "valid_steps: 500\n",
    "valid_metrics: [\"BLEU\"]\n",
    "\n",
    "# # Early Stop\n",
    "# early_stopping: 10\n",
    "# early_stopping_criteria: \"accuracy\"\n",
    "\n",
    "# early_stopping:\n",
    "#     # (optional) The target metric name (default: \"loss\").\n",
    "#     metric: bleu\n",
    "#     # (optional) The metric should improve at least by this much to be considered\n",
    "#     # as an improvement (default: 0)\n",
    "#     min_improvement: 0.01\n",
    "#     steps: 10\n",
    "\n",
    "# Default: 4000 - for large datasets, try up to 8000\n",
    "warmup_steps: 1000\n",
    "report_every: 100\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "bucket_size: 17096 # whole corpus\n",
    "num_workers: 8  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"sents\"\n",
    "batch_size: 128   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "optim: \"adam\"\n",
    "learning_rate: 2\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.998\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"sents\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "layers: 2\n",
    "heads: 2\n",
    "word_vec_size: 128\n",
    "hidden_size: 128\n",
    "transformer_ff: 512\n",
    "dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "\n",
    "'''\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(config_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare-bones base config\n",
    "base_config = \"\"\"\n",
    "save_data: run\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    corpus_1:\n",
    "        path_src: src-train.txt\n",
    "        path_tgt: tgt-train.txt\n",
    "    valid:\n",
    "        path_src: src-val.txt\n",
    "        path_tgt: tgt-val.txt\n",
    "\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: src.vocab\n",
    "tgt_vocab: tgt.vocab\n",
    "\n",
    "# Where to save the log file and the output models/checkpoints\n",
    "log_file: train.log\n",
    "save_model: models_scratch/modelbase\n",
    "\n",
    "train_steps: 10000\n",
    "report_every: 100\n",
    "\n",
    "# # Default: 10000 - Run validation after n steps\n",
    "# # once every 78 \n",
    "valid_steps: 1000\n",
    "valid_metrics: [\"BLEU\"]\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "bucket_size: 17096 # whole corpus\n",
    "num_workers: 8  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"sents\"\n",
    "batch_size: 128   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "optim: \"adam\"\n",
    "learning_rate: 2\n",
    "decay_method: \"noam\"\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.998\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"sents\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"base_config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(base_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model evaluated after every 9984 steps (128 batch size * 78 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24424\n"
     ]
    }
   ],
   "source": [
    "unique_tok = defaultdict(set)\n",
    "with open(\"Sentence pairs in English-Toki Pona - 2024-04-02.tsv\", \"r\") as pairFile:\n",
    "    for pair in pairFile:\n",
    "        l=pair.split('\\t')\n",
    "        en = l[1]\n",
    "        tok = l[3]\n",
    "        en = en.lower()\n",
    "        tok = tok.lower()\n",
    "        en = remove_punct(en)\n",
    "        tok = remove_punct(tok)\n",
    "        unique_tok[tok].add(en)\n",
    "pairs = []\n",
    "for en, targets in unique_tok.items():\n",
    "  # remove ambiguous sentences\n",
    "  if len(targets) == 1:\n",
    "    pairs.append((en, list(targets)[0]))\n",
    "  \n",
    "# import random\n",
    "\n",
    "# random.shuffle(pairs)\n",
    "\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src.txt\", \"w\") as srcFile:\n",
    "  with open(\"tgt.txt\", \"w\") as tgtFile:\n",
    "    for l in pairs:\n",
    "      srcFile.write(l[0])\n",
    "      tgtFile.write(l[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442\n",
      "17096\n",
      "4886\n",
      "2442\n",
      "17096\n",
      "4886\n"
     ]
    }
   ],
   "source": [
    "def splitSrcTarget(srcName,valName,trainName,evalName,train_size,val_size):\n",
    "  with open(srcName, \"r\") as srcFile:\n",
    "    with open(valName, \"w\") as srcVal:\n",
    "      i = 0\n",
    "      for sent in srcFile:\n",
    "        srcVal.write(sent)\n",
    "        i += 1\n",
    "        if i >= val_size:\n",
    "          print(i)\n",
    "          break\n",
    "    with open(trainName, \"w\") as srcTrain:\n",
    "      i = 0\n",
    "      for sent in srcFile:\n",
    "        srcTrain.write(sent)\n",
    "        i += 1\n",
    "        if i >= train_size:\n",
    "          print(i)\n",
    "          break\n",
    "      \n",
    "    with open(evalName, \"w\") as srcEval:\n",
    "      i = 0\n",
    "      for sent in srcFile:\n",
    "        srcEval.write(sent)\n",
    "        i += 1\n",
    "      print(i)\n",
    "splitSrcTarget(\"src.txt\",\"src-val.txt\",\"src-train.txt\",\"src-eval.txt\",17096,2442) # 70%, 10%, 20% \n",
    "splitSrcTarget(\"tgt.txt\",\"tgt-val.txt\",\"tgt-train.txt\",\"tgt-eval.txt\",17096,2442) # 70%, 10%, 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2024-04-03 19:52:54,240 INFO] Counter vocab from -1 samples.\n",
      "[2024-04-03 19:52:54,240 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2024-04-03 19:52:54,576 INFO] Counters src: 934\n",
      "[2024-04-03 19:52:54,576 INFO] Counters tgt: 7088\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the vocab breaks, hmm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpa4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
