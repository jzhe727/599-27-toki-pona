[2024-04-02 03:13:40,825 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:13:40,826 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:13:40,826 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:13:40,828 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:13:40,829 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:21:50,997 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:21:50,998 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:21:50,998 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:21:50,999 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:21:51,000 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:27:43,457 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:27:43,457 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:27:43,458 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:27:43,459 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:27:43,459 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:27:43,474 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 03:27:43,474 INFO] The decoder start token is: <s>
[2024-04-02 03:27:43,475 INFO] Building model...
[2024-04-02 03:27:51,423 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 03:27:51,423 INFO] Non quantized layer compute is fp16
[2024-04-02 03:27:51,730 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=512, out_features=6824, bias=True)
)
[2024-04-02 03:27:51,731 INFO] encoder: 3267584
[2024-04-02 03:27:51,732 INFO] decoder: 9624232
[2024-04-02 03:27:51,733 INFO] * number of parameters: 12891816
[2024-04-02 03:27:51,733 INFO] Trainable parameters = {'torch.float32': 12891816, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:27:51,734 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:27:51,734 INFO]  * src vocab size = 912
[2024-04-02 03:27:51,734 INFO]  * tgt vocab size = 6824
[2024-04-02 03:34:41,823 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:34:41,824 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:34:41,824 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:34:41,825 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:34:41,826 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:34:41,838 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 03:34:41,838 INFO] The decoder start token is: <s>
[2024-04-02 03:34:41,839 INFO] Building model...
[2024-04-02 03:34:42,751 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 03:34:42,752 INFO] Non quantized layer compute is fp16
[2024-04-02 03:34:42,905 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=500, out_features=500, bias=False)
          (linear_values): Linear(in_features=500, out_features=500, bias=False)
          (linear_query): Linear(in_features=500, out_features=500, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=500, out_features=500, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=500, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=500, bias=False)
          (layer_norm): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=500, out_features=500, bias=False)
          (linear_values): Linear(in_features=500, out_features=500, bias=False)
          (linear_query): Linear(in_features=500, out_features=500, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=500, out_features=500, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=500, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=500, bias=False)
          (layer_norm): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=500, out_features=500, bias=False)
          (linear_values): Linear(in_features=500, out_features=500, bias=False)
          (linear_query): Linear(in_features=500, out_features=500, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=500, out_features=500, bias=False)
        )
        (layer_norm_2): LayerNorm((500,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=500, out_features=6824, bias=True)
)
[2024-04-02 03:34:42,907 INFO] encoder: 3145736
[2024-04-02 03:34:42,907 INFO] decoder: 9323296
[2024-04-02 03:34:42,908 INFO] * number of parameters: 12469032
[2024-04-02 03:34:42,909 INFO] Trainable parameters = {'torch.float32': 12469032, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:34:42,910 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:34:42,911 INFO]  * src vocab size = 912
[2024-04-02 03:34:42,911 INFO]  * tgt vocab size = 6824
[2024-04-02 03:38:22,368 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:38:22,369 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:38:22,369 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:38:22,370 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:38:22,371 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:38:22,383 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 03:38:22,383 INFO] The decoder start token is: <s>
[2024-04-02 03:38:22,383 INFO] Building model...
[2024-04-02 03:38:23,176 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 03:38:23,177 INFO] Non quantized layer compute is fp16
[2024-04-02 03:38:23,328 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=512, out_features=6824, bias=True)
)
[2024-04-02 03:38:23,329 INFO] encoder: 3267584
[2024-04-02 03:38:23,330 INFO] decoder: 9624232
[2024-04-02 03:38:23,330 INFO] * number of parameters: 12891816
[2024-04-02 03:38:23,331 INFO] Trainable parameters = {'torch.float32': 12891816, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:38:23,332 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:38:23,332 INFO]  * src vocab size = 912
[2024-04-02 03:38:23,333 INFO]  * tgt vocab size = 6824
[2024-04-02 03:39:09,803 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 03:39:09,803 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 03:39:09,804 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 03:39:09,804 INFO] Parsed 2 corpora from -data.
[2024-04-02 03:39:09,805 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 03:39:09,817 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 03:39:09,817 INFO] The decoder start token is: <s>
[2024-04-02 03:39:09,818 INFO] Building model...
[2024-04-02 03:39:10,567 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 03:39:10,567 INFO] Non quantized layer compute is fp16
[2024-04-02 03:39:10,703 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 03:39:10,704 INFO] encoder: 511232
[2024-04-02 03:39:10,705 INFO] decoder: 2279848
[2024-04-02 03:39:10,706 INFO] * number of parameters: 2791080
[2024-04-02 03:39:10,706 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:39:10,707 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 03:39:10,707 INFO]  * src vocab size = 912
[2024-04-02 03:39:10,707 INFO]  * tgt vocab size = 6824
[2024-04-02 04:23:39,803 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:23:39,804 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:23:39,804 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:23:39,805 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:23:39,805 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:23:39,817 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:23:39,818 INFO] The decoder start token is: <s>
[2024-04-02 04:23:39,818 INFO] Building model...
[2024-04-02 04:23:40,642 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:23:40,643 INFO] Non quantized layer compute is fp16
[2024-04-02 04:23:40,779 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:23:40,781 INFO] encoder: 511232
[2024-04-02 04:23:40,781 INFO] decoder: 2279848
[2024-04-02 04:23:40,781 INFO] * number of parameters: 2791080
[2024-04-02 04:23:40,782 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:23:40,782 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:23:40,783 INFO]  * src vocab size = 912
[2024-04-02 04:23:40,783 INFO]  * tgt vocab size = 6824
[2024-04-02 04:26:51,661 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:26:51,662 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:26:51,662 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:26:51,663 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:26:51,664 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:26:51,677 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:26:51,677 INFO] The decoder start token is: <s>
[2024-04-02 04:26:51,678 INFO] Building model...
[2024-04-02 04:26:52,510 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:26:52,511 INFO] Non quantized layer compute is fp16
[2024-04-02 04:26:52,635 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:26:52,637 INFO] encoder: 511232
[2024-04-02 04:26:52,637 INFO] decoder: 2279848
[2024-04-02 04:26:52,638 INFO] * number of parameters: 2791080
[2024-04-02 04:26:52,638 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:26:52,638 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:26:52,639 INFO]  * src vocab size = 912
[2024-04-02 04:26:52,639 INFO]  * tgt vocab size = 6824
[2024-04-02 04:27:36,113 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:27:36,113 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:27:36,114 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:27:36,115 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:27:36,117 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:27:36,131 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:27:36,131 INFO] The decoder start token is: <s>
[2024-04-02 04:27:36,132 INFO] Building model...
[2024-04-02 04:27:36,684 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:27:36,684 INFO] Non quantized layer compute is fp16
[2024-04-02 04:27:36,817 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:27:36,818 INFO] encoder: 511232
[2024-04-02 04:27:36,819 INFO] decoder: 2279848
[2024-04-02 04:27:36,819 INFO] * number of parameters: 2791080
[2024-04-02 04:27:36,820 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:27:36,820 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:27:36,820 INFO]  * src vocab size = 912
[2024-04-02 04:27:36,821 INFO]  * tgt vocab size = 6824
[2024-04-02 04:27:53,056 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:27:53,057 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:27:53,057 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:27:53,058 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:27:53,058 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:27:53,070 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:27:53,070 INFO] The decoder start token is: <s>
[2024-04-02 04:27:53,071 INFO] Building model...
[2024-04-02 04:27:53,824 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:27:53,824 INFO] Non quantized layer compute is fp16
[2024-04-02 04:27:53,955 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:27:53,957 INFO] encoder: 511232
[2024-04-02 04:27:53,957 INFO] decoder: 2279848
[2024-04-02 04:27:53,958 INFO] * number of parameters: 2791080
[2024-04-02 04:27:53,959 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:27:53,959 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:27:53,960 INFO]  * src vocab size = 912
[2024-04-02 04:27:53,960 INFO]  * tgt vocab size = 6824
[2024-04-02 04:28:03,867 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:28:03,867 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:28:03,868 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:28:03,870 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:28:03,871 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:28:03,885 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:28:03,885 INFO] The decoder start token is: <s>
[2024-04-02 04:28:03,886 INFO] Building model...
[2024-04-02 04:28:04,493 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:28:04,493 INFO] Non quantized layer compute is fp16
[2024-04-02 04:28:04,623 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:28:04,624 INFO] encoder: 511232
[2024-04-02 04:28:04,625 INFO] decoder: 2279848
[2024-04-02 04:28:04,625 INFO] * number of parameters: 2791080
[2024-04-02 04:28:04,626 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:28:04,626 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:28:04,627 INFO]  * src vocab size = 912
[2024-04-02 04:28:04,627 INFO]  * tgt vocab size = 6824
[2024-04-02 04:28:14,257 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:28:14,258 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:28:14,258 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:28:14,258 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:28:14,258 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:28:14,270 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:28:14,270 INFO] The decoder start token is: <s>
[2024-04-02 04:28:14,271 INFO] Building model...
[2024-04-02 04:28:14,813 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:28:14,813 INFO] Non quantized layer compute is fp16
[2024-04-02 04:28:14,941 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:28:14,943 INFO] encoder: 511232
[2024-04-02 04:28:14,943 INFO] decoder: 2279848
[2024-04-02 04:28:14,944 INFO] * number of parameters: 2791080
[2024-04-02 04:28:14,944 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:28:14,944 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:28:14,945 INFO]  * src vocab size = 912
[2024-04-02 04:28:14,945 INFO]  * tgt vocab size = 6824
[2024-04-02 04:29:18,413 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:29:18,413 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:29:18,413 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:29:18,414 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:29:18,415 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:29:18,427 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:29:18,427 INFO] The decoder start token is: <s>
[2024-04-02 04:29:18,428 INFO] Building model...
[2024-04-02 04:29:19,141 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:29:19,141 INFO] Non quantized layer compute is fp16
[2024-04-02 04:29:19,264 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:29:19,265 INFO] encoder: 511232
[2024-04-02 04:29:19,266 INFO] decoder: 2279848
[2024-04-02 04:29:19,266 INFO] * number of parameters: 2791080
[2024-04-02 04:29:19,267 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:29:19,267 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:29:19,268 INFO]  * src vocab size = 912
[2024-04-02 04:29:19,268 INFO]  * tgt vocab size = 6824
[2024-04-02 04:29:31,234 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:29:31,234 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:29:31,235 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:29:31,235 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:29:31,236 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:29:31,249 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:29:31,249 INFO] The decoder start token is: <s>
[2024-04-02 04:29:31,249 INFO] Building model...
[2024-04-02 04:29:31,795 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:29:31,795 INFO] Non quantized layer compute is fp16
[2024-04-02 04:29:31,920 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:29:31,921 INFO] encoder: 511232
[2024-04-02 04:29:31,922 INFO] decoder: 2279848
[2024-04-02 04:29:31,923 INFO] * number of parameters: 2791080
[2024-04-02 04:29:31,924 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:29:31,924 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:29:31,925 INFO]  * src vocab size = 912
[2024-04-02 04:29:31,925 INFO]  * tgt vocab size = 6824
[2024-04-02 04:41:23,709 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:41:23,710 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:41:23,710 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:41:23,711 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:41:23,712 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:41:23,724 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:41:23,724 INFO] The decoder start token is: <s>
[2024-04-02 04:41:23,724 INFO] Building model...
[2024-04-02 04:41:24,486 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:41:24,486 INFO] Non quantized layer compute is fp16
[2024-04-02 04:41:24,617 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:41:24,618 INFO] encoder: 511232
[2024-04-02 04:41:24,619 INFO] decoder: 2279848
[2024-04-02 04:41:24,619 INFO] * number of parameters: 2791080
[2024-04-02 04:41:24,620 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:41:24,620 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:41:24,621 INFO]  * src vocab size = 912
[2024-04-02 04:41:24,621 INFO]  * tgt vocab size = 6824
[2024-04-02 04:45:23,368 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:45:23,369 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:45:23,369 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:45:23,370 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:45:23,371 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:45:23,383 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:45:23,383 INFO] The decoder start token is: <s>
[2024-04-02 04:45:23,384 INFO] Building model...
[2024-04-02 04:45:24,124 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:45:24,124 INFO] Non quantized layer compute is fp16
[2024-04-02 04:45:24,256 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:45:24,257 INFO] encoder: 511232
[2024-04-02 04:45:24,258 INFO] decoder: 2279848
[2024-04-02 04:45:24,259 INFO] * number of parameters: 2791080
[2024-04-02 04:45:24,260 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:45:24,260 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:45:24,261 INFO]  * src vocab size = 912
[2024-04-02 04:45:24,261 INFO]  * tgt vocab size = 6824
[2024-04-02 04:45:43,317 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:45:43,317 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:45:43,317 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:45:43,318 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:45:43,318 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:45:43,331 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:45:43,331 INFO] The decoder start token is: <s>
[2024-04-02 04:45:43,332 INFO] Building model...
[2024-04-02 04:45:43,877 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:45:43,877 INFO] Non quantized layer compute is fp16
[2024-04-02 04:45:44,004 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:45:44,006 INFO] encoder: 511232
[2024-04-02 04:45:44,006 INFO] decoder: 2279848
[2024-04-02 04:45:44,007 INFO] * number of parameters: 2791080
[2024-04-02 04:45:44,007 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:45:44,008 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:45:44,008 INFO]  * src vocab size = 912
[2024-04-02 04:45:44,009 INFO]  * tgt vocab size = 6824
[2024-04-02 04:57:01,611 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 04:57:01,611 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 04:57:01,612 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 04:57:01,613 INFO] Parsed 2 corpora from -data.
[2024-04-02 04:57:01,613 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 04:57:01,627 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 04:57:01,627 INFO] The decoder start token is: <s>
[2024-04-02 04:57:01,628 INFO] Building model...
[2024-04-02 04:57:02,609 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 04:57:02,610 INFO] Non quantized layer compute is fp16
[2024-04-02 04:57:02,737 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 04:57:02,739 INFO] encoder: 511232
[2024-04-02 04:57:02,740 INFO] decoder: 2279848
[2024-04-02 04:57:02,740 INFO] * number of parameters: 2791080
[2024-04-02 04:57:02,741 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:57:02,741 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 04:57:02,742 INFO]  * src vocab size = 912
[2024-04-02 04:57:02,742 INFO]  * tgt vocab size = 6824
[2024-04-02 05:09:06,675 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 05:09:06,676 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 05:09:06,676 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 05:09:06,677 INFO] Parsed 2 corpora from -data.
[2024-04-02 05:09:06,677 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 05:09:06,690 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 05:09:06,690 INFO] The decoder start token is: <s>
[2024-04-02 05:09:06,690 INFO] Building model...
[2024-04-02 05:09:07,467 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 05:09:07,467 INFO] Non quantized layer compute is fp16
[2024-04-02 05:09:07,603 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 05:09:07,604 INFO] encoder: 511232
[2024-04-02 05:09:07,604 INFO] decoder: 2279848
[2024-04-02 05:09:07,605 INFO] * number of parameters: 2791080
[2024-04-02 05:09:07,606 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 05:09:07,606 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 05:09:07,607 INFO]  * src vocab size = 912
[2024-04-02 05:09:07,607 INFO]  * tgt vocab size = 6824
[2024-04-02 05:11:48,174 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-02 05:11:48,174 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-02 05:11:48,174 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-02 05:11:48,175 INFO] Parsed 2 corpora from -data.
[2024-04-02 05:11:48,176 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-02 05:11:48,188 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'li', 'e', 'mi', 'jan', 'ni', 'tawa']
[2024-04-02 05:11:48,188 INFO] The decoder start token is: <s>
[2024-04-02 05:11:48,188 INFO] Building model...
[2024-04-02 05:11:48,867 INFO] Switching model to float32 for amp/apex_amp
[2024-04-02 05:11:48,867 INFO] Non quantized layer compute is fp16
[2024-04-02 05:11:48,986 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(912, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(6824, 128, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=False)
          (w_2): Linear(in_features=512, out_features=128, bias=False)
          (layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=128, out_features=128, bias=False)
          (linear_values): Linear(in_features=128, out_features=128, bias=False)
          (linear_query): Linear(in_features=128, out_features=128, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=128, out_features=128, bias=False)
        )
        (layer_norm_2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=128, out_features=6824, bias=True)
)
[2024-04-02 05:11:48,988 INFO] encoder: 511232
[2024-04-02 05:11:48,989 INFO] decoder: 2279848
[2024-04-02 05:11:48,989 INFO] * number of parameters: 2791080
[2024-04-02 05:11:48,990 INFO] Trainable parameters = {'torch.float32': 2791080, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 05:11:48,990 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-04-02 05:11:48,990 INFO]  * src vocab size = 912
[2024-04-02 05:11:48,991 INFO]  * tgt vocab size = 6824
[2024-04-03 16:07:40,704 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-03 16:07:40,706 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-03 16:07:40,706 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-03 16:07:40,707 INFO] Parsed 2 corpora from -data.
[2024-04-03 16:07:40,708 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2024-04-03 16:18:26,821 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2024-04-03 16:18:26,824 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-04-03 16:18:26,824 INFO] Missing transforms field for valid data, set to default: [].
[2024-04-03 16:18:26,825 INFO] Parsed 2 corpora from -data.
[2024-04-03 16:18:26,826 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
